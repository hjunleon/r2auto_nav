Post project notes:

Code was largely working, but it didn’t account for overloading of the rpi. E.g. LIDAR update rate dropped to about 2Hz or less, causing SLAM map to drift

Code will work wondrously if my obstacle collision mechanism is better. Given the time constraint and my lack of ability to work under pressure and time constraint (i’ve got to work on this), discretising obstacle detection to occupancy grids only is not ideal for small maps / low resolution maps. I feel that even if i changed my footprint collision to continuous x,y, lidar information would be necessary after all. Therefore, Lidar information shouldn’t only be applied to recovery methods (as this is triggered every time the normal obstacle detection fails), but also to the general path planning and traversing.  This is where, in my opinion, navstack fails as well because the entire local planner didn’t even account for data that’s frankly more reliable. And this is understandable too, because navstack is too general and local planners are usually fine with the costmap so long as it’s high resolution enough. (Thus relying on the individual to create accurate global costmap and local costmap maybe with their own custom map server)


In short, one approach to account for such low resolution, low refresh rate occupancy grids is to make use of sensors of higher frequency to build or evaluate the local costmap. 2 ways to do this is maybe subscribing to the submap node and hoping it refreshes fast. The other way is, whatever i have done so far (A star and discretised-by-cell costmap footprint)  can be a heuristic, and if this heuristic fails, engage lidar based trajectory prediction.



Regarding frontier detection, I am honestly very proud of it. It really proved that my understanding of machine learning, Exploratory Data Analysis and most importantly feature engineering has not been fluff haha. However this is also largely based on the SLAM algorithm (cartographer)that enabled me to fine tune my histogram-binning method. However, I believe there must be better ways to do this still but i'm not sure.


For the path planning, frankly any works. I would like to explore D star and RRT-based algorithms. I acknowledge A star is not that ideal for our use case because it loves to hug obstacles, and our bot has trouble dealing with this already. I believe the combination of the local planner and A star as the global planner, I created an implementation of Hybrid A star by accident? 

Last but not least, the part I ripped off navstack (which includes the obstacle collision detector mentioned in paragraph 2), the local planner. I’m genuinely proud to have not only understand the algorithm, but also to understand how to apply it in different scenarios(e.g. Moving obstacles in computer games, cell-based and continuous maps)  and to take the parts I want from navstack and modify accordingly for our use case. However, the obstacle cost wasn’t the only thing that suffered in this implementation, but the distance away from ideal path as well. Straight paths are understood ok, but curved paths with the cell-interpolation may have led to some issues? 

Other miscellaneous stuff I picked up (but not necessarily implemented) are unit testing skills (learnt primarily from navstack), ROS, network-based computation, and the importance to account for high latency and low-computation for publisher-subscription mechanisms. As a web developer, websites running on a single thread can handle these with ease, but only because commercial computers are powerful enough, and most importantly the server as well. The RPI is a pretty weak client, which is a reminder to evaluate each client’s limitations and adjust publishing and subscription rates accordingly. Maybe time how long it takes to print a succession of statements on initialization and transmit that to the server? That way I do not have to reconfigure my parameters manually. Unit testing is important, because I found myself lost lots of times as to where my code is failing. Even if there’s a function traceback, testing on known inputs and outputs that are independent of other modules is less of a headache. Other skills I’ve learnt is to understand open source code better. This is my second proper attempt to understand semi- big open source projects, first being youtube-dl. I believe I became better at finding entry points and finding dependencies of different modules (class inheritance, class variables that are instances of other classes). Then there’s the importance of parallel programming. My code is single threaded most likely, so it struggles to achieve the local planner every 75ms and below. However, navstack’s implementation is not only in C++, but also relies on mutexes and threading. I understand them only briefly, but definitely something worth understanding alot deeper considering algorithm optimisation in real-world scenarios often become micro optimisations on top of the architecture engineers design. E.g. navstack compute dijkstra's faster than my code computes A star. Maybe another thing is to learn that pseudocode and implementation can be very different so long as they achieve the implementation follows the high-level details in the psedocode. E.g. DWA for navstack is very complicated compared to a single-file pygame demonstration, despite achieving the same DWA algorithm. 


Conclusion: Learnt alot about robotics, software and hardware integration is hard, but all in all really enjoyed this module. It is better to fail than to not learn anything at all, and I strongly feel my investment in this project has paid off. I’m close to being opencv expert at this point haha, which is needed for my coming summer internship so I’m glad this module has been so relevant to me :D Thanks profs and friends for the support! 
